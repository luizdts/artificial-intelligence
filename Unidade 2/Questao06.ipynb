{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Questão 06\n",
        "Pesquise sobre redes neurais recorrentes LSTM. Apresente neste estudo aplicações das \n",
        "LSTM deep learning. Seguem abaixo sugestões de aplicações.\n",
        "\n",
        "***i) Processamento de Linguagem Natural***\n",
        "\n",
        "ii) Conversão voz/texto\n",
        "\n",
        "iii) Tradução de textos\n",
        "\n",
        "iv) outras aplicações\n"
      ],
      "metadata": {
        "id": "GI4osYvL8t1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Utilizando o tema de Processamento de Linguagem Natural, iremos analisar uma aplicação que faz uso do LSTM deep learning que faz a previsão de quatro categorias de problemas de saúde mental: depressão, alcoolismo, drogas e suicídio.\n",
        "\n",
        "- O dataset desse projeto é composto por declarações e perguntas expressas por estudantes de diversas universidades do Quênia, onde relataram sofrer com esses desafios de saúde mental.\n",
        "\n",
        "- O projeto está disponível no seguinte [link](https://dadosaocubo.com/processamento-de-linguagem-natural-com-tensorflow/)."
      ],
      "metadata": {
        "id": "ZZ5-pnAE1snm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Carregando Bibliotecas e Dataset\n",
        "Primeiramente vamos começar importando todas as bibliotecas para rodar o nosso modelo. Destaque para as biblioteca Pandas, Numpy, NLTK e TensorFlow! Pandas utilizamos para importar e organizar os nossos dados em formato de data frame. Numpy utilizamos para fazer algumas conversões importantes com nossos dados. NLTK utilizamos para trabalhar com o texto, muito poderosa para problemas de NLP. Por fim o TensorFlow que utilizamos para o nosso modelo de machine learning. As demais bibliotecas foram utilizadas de forma auxiliar e vou te explicar no decorrer do código.\n",
        "\n",
        "# Importação das bibliotecas"
      ],
      "metadata": {
        "id": "KNegH9Ej3gGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja >log"
      ],
      "metadata": {
        "id": "wWM3GPXf9C8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wlTORQM8bM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cc77fb-68e4-46e1-e16d-751b47cd2a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Importação das bibliotecas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import wordninja\n",
        "import textblob\n",
        "import collections\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após as bibliotecas importadas vamos carregar o dataset para começar a trabalhar os dados."
      ],
      "metadata": {
        "id": "g1tUx6yE3YB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importação dos dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/dadosaocubo/nlp/master/mental_health.csv')"
      ],
      "metadata": {
        "id": "0w1ENTot3QCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparação dos Dados\n",
        "\n",
        "Vamos começar a preparar os dados para serem consumidos pelo modelo. Então, criamos algumas variáveis que vamos utilizar mais a frente. A lista de palavras vazias (stop_words), o tamanho da base para teste (test_size) e duas variáveis para o modelo quantidade de épocas (epochs) e número de exemplos por lote (batch_size)."
      ],
      "metadata": {
        "id": "aH9giP8_3U_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Palavras para retirar da análise\n",
        "stop_words = stopwords.words('english')\n",
        "# Variável tamanho da base de teste\n",
        "test_size = 0.1\n",
        "# Variáveis do modelo\n",
        "epochs = 10\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "Zl20Z7ko3Szq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos começar a tratar os nosso dados, fazer aquela faxina. Dessa forma o “lixo” que vamos tirar, não vai atrapalhar o nosso modelo.\n",
        "\n",
        "#Limpeza dos Dados\n",
        "\n",
        "O primeiro passo separar as possíveis palavras concatenadas. Para essa tarefa criamos a coluna text_split e aplicamos a função split da biblioteca wordninja. Na sequência criamos a coluna text_new e aplicamos a função TreebankWordDetokenizer().detokenize para formar novamente um texto com as palavras separadas."
      ],
      "metadata": {
        "id": "KdATqovB3jy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando palavras juntas\n",
        "df['text_split'] = df['text'].apply(wordninja.split)\n",
        "df['text_new'] = df['text_split'].apply(TreebankWordDetokenizer().detokenize)"
      ],
      "metadata": {
        "id": "0qA5BCut3nVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos corrigir palavras escritas de forma incorreta. Dessa vez vamos aplicar a função textblob.TextBlob.correct da biblioteca textblob na mesma coluna text_new . Observe que já existe muita coisa pronta em python, basta saber encontrar para utilizar."
      ],
      "metadata": {
        "id": "3dkJrxES3nBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrigindo palavras incorretas\n",
        "df['text_new'] = df['text_new'].apply(textblob.TextBlob).apply(textblob.TextBlob.correct).apply(str)"
      ],
      "metadata": {
        "id": "wcV2cJ2O3qck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para o tratamento de números, pontuação e caracteres especiais utilizamos expressões regulares sempre aplicando na coluna text_new. No fim deste bloco aplicamos a função lower para deixar todas as letras e caixa baixa."
      ],
      "metadata": {
        "id": "vsUYIRcY3tFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluindo da descrição os números.\n",
        "df['text_new'] = df['text_new'].str.replace('[0-9]+', '', regex=True).copy()\n",
        "# Excluindo da descrição pontuação.\n",
        "df['text_new'] = df['text_new'].str.replace('[,.:;!?]+', ' ', regex=True).copy()\n",
        "# Excluindo da descrição caracteres especiais.\n",
        "df['text_new'] = df['text_new'].str.replace('[/<>()|\\+\\-\\$%&#@\\'\\\"]+', ' ', regex=True).copy()\n",
        "# Colocando todos os caracteres em caixa baixa.\n",
        "df['text_new'] = df['text_new'].str.lower().copy()"
      ],
      "metadata": {
        "id": "be_tDOrj3uU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, na limpeza de dados vamos remover as palavras vazias, mais conhecidas como stop words. Para essa tarefa, criei a função tokenize_df (Faltou um pouco de criatividade no nome :-)) para remover as palavras vazias e retornar somente o conteúdo útil do texto."
      ],
      "metadata": {
        "id": "q9zSqATV3wqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para retirar stop words\n",
        "def tokenize_df(tokenized_words):\n",
        "  tokenized_words = word_tokenize(tokenized_words)\n",
        "  stop = [word for word in tokenized_words if word not in stop_words]\n",
        "  text = TreebankWordDetokenizer().detokenize(stop)\n",
        "  return text\n",
        "# Eliminando as stop words\n",
        "df['text_new'] = df['text_new'].apply(tokenize_df).copy()"
      ],
      "metadata": {
        "id": "ImeTml9131Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A cada etapa dessa realizada acima, podemos exibir as primeiras linhas do dataframe com a função head para ver se cada etapa está sendo feita corretamente. No notebook que que vou disponibilizar no final foi feito essa análise.\n",
        "\n",
        "Ao final dessa limpeza estamos pronto para o input no modelo? Não! Precisamos transformar nossa coluna text_new em objetos do tipo tensor para que o modelo entenda os dados. Vamos ver como fazer essa transformação!\n",
        "\n",
        "#Transformação para Input no Modelo\n",
        "Vamos começar gerando uma lista de palavras únicas de todas as declarações e perguntas das base após a limpeza."
      ],
      "metadata": {
        "id": "via5zfn639H2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionando as únicas palavras da variável text_new\n",
        "df['text_new_split'] = df['text_new'].apply(word_tokenize).copy()\n",
        "text = list(df.text_new_split)\n",
        "list_words = [item for sublist in text for item in sublist]\n",
        "list_words = sorted(list_words)\n",
        "only_words = set(list_words)"
      ],
      "metadata": {
        "id": "OsSsZlWz4A_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com o código acima criamos uma lista com todas as palavras na variável list_words e com a função set removemos as palavras duplicadas gerando a variável only_words apenas com as palavras únicas.\n",
        "\n",
        "Então, vamos gerar o nosso codificador de palavras com o vocabulário de palavras únicas q criamos assim. Para isso vamos usar a função SubwordTextEncoder(vocab_list=only_words), passando como parâmetro a nossa lista de palavras únicas only_words."
      ],
      "metadata": {
        "id": "9NzAPM-S4Dkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gerando encoder com o vocabulário das palavras\n",
        "encoder = tfds.deprecated.text.SubwordTextEncoder(vocab_list=only_words)"
      ],
      "metadata": {
        "id": "CIm5L32g4EOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O encoder acima foi para a nossa feature, ou seja, nossa variável de entrada. Precisamos agora fazer o codificador do target ou nossa variável de saída. Vamos usar a função do scikit-learn LabelEncoder, como pode ver no código abaixo."
      ],
      "metadata": {
        "id": "SCTw-dtb4Gpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode do label\n",
        "label_encode = LabelEncoder()\n",
        "target = label_encode.fit_transform(df['label'])"
      ],
      "metadata": {
        "id": "jBD0BJWm4HEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora precisamos definir as nossas variáveis de feature e target, como podemos ver no código abaixo representadas como x e y. Ainda usando o scikit-learn com a função train_test_split dividimos os dados em treino e teste para fazer uma avaliação de performance do modelo. Lembra da variável test_size que definimos lá no ínicio do item 4.? Agora vamos usar ela para definir o tamanho dos dados de teste."
      ],
      "metadata": {
        "id": "Ubq-dffU4JjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo feature e target\n",
        "x = df['text_new']\n",
        "y = target\n",
        "# Dividindo dataset em treino e teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)"
      ],
      "metadata": {
        "id": "UFrnlfxB4K20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim na transformação do input, vamos converter os dados de entradas para o formato de tensor.\n",
        "\n",
        "Para essa etapa crie duas funções. A função pad_to_size que define o tamanho do vetor e complementa com zeros. E a função encode_input que pega o texto, converte em vetor usando a função encoder (que falei um pouco mais acima) completa o vetor com zeros com a função pad_to_size e por fim transforma esse vetor em um tensor com a função tf.cast."
      ],
      "metadata": {
        "id": "lr91FLX04MHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para criação da matriz\n",
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec\n",
        "# Função para encode do input\n",
        "def encode_input(text_new):\n",
        "  list_x = []\n",
        "  for text in text_new:\n",
        "    text_encode = encoder.encode(text)\n",
        "    text_encode = pad_to_size(text_encode, 64)\n",
        "    list_x.append(text_encode)\n",
        "  # Convertendo x em tensor\n",
        "  input_encode = tf.cast(list_x, tf.float32)\n",
        "  return input_encode\n",
        "# Encode do x_train e x_test\n",
        "x_train = encode_input(x_train)\n",
        "x_test = encode_input(x_test)"
      ],
      "metadata": {
        "id": "d0-POJdQ4NuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Então saímos com os dados prontos para o input no modelo. Agora vamos ver como criar esse modelo.\n",
        "\n",
        "#Modelo do TensorFlow\n",
        "Para a resolução do problema vamos utilizar uma Rede Neural Recorrente. Agora preciso em poucas palavras te dizer o que é uma RNN.\n",
        "\n",
        "Para definir uma RNN vamos ver a definição encontrada no Machine Learning Glossary do Google, adaptada e traduzida:\n",
        "\n",
        "“Uma rede neural que é executada intencionalmente várias vezes, em que partes de cada execução alimentam a próxima execução. Especificamente, as camadas ocultas da execução anterior fornecem parte da entrada para a mesma camada oculta na execução seguinte. As redes neurais recorrentes são particularmente úteis para avaliar sequências, para que as camadas ocultas possam aprender com as execuções anteriores da rede neural em partes anteriores da sequência.”\n",
        "\n",
        "De uma forma bruta é uma rede que tem como entrada nas camadas ocultas a própria saída de forma intencional até encontrar a equação matemática desejada.\n",
        "\n",
        "As redes RNN são boas em resolver problemas de NLP, então criamos uma com 6 camadas, e vamos detalhar um pouco mais essas camadas a seguir."
      ],
      "metadata": {
        "id": "NvCAEfiV4uzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criação do modelo de RNN\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(encoder.vocab_size, 128)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oseal8cJ44nP",
        "outputId": "453e1f54-b83d-46e8-b870-80879c12fb03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 128)         123904    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 128)        98816     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 322,052\n",
            "Trainable params: 322,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com a função summary é possível ter um resumo do modelo criado como podemos ver na imagem acima."
      ],
      "metadata": {
        "id": "rqzISke448jW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Input no TensorFlow\n",
        "É uma camada obrigatória em todos os modelos, é a camada que recebe os dados de entrada.\n",
        "\n",
        "#Embedding no TensorFlow\n",
        "É a camada onde é feita a tradução do vetor de entrada em alta dimensão para uma para um espaço de baixa dimensão. \n",
        "\n",
        "#LSTM no TensorFlow\n",
        "LSTM é a abreviação de Long Short-Term Memory em tradução livre Memória Longa de Curto Prazo. É a camada responsável por armazenar em memória a informação das camadas anteriores, mantendo um histórico das novas entradas para melhorar a performance do modelo. \n",
        "\n",
        "#Dropout no TensorFlow\n",
        "Esta é uma camada de regularização da rede, que aleatoriamente exclui um número fixo de unidades em uma camada, para forçar uma regularização do modelo.\n",
        "\n",
        "#Dense no TensorFlow\n",
        "É a nossa camada de saída, a qual está conectadas a todos os nós das camada ocultas anteriores.\n",
        "\n",
        "#Treinamento do Modelo\n",
        "Com o nosso modelo pronto, podemos compilar com a função compile. Onde é definido a função de perda com o parâmetro loss, a função de otimização com o parâmetro optimizer e a métrica com o parâmetro metrics.\n",
        "\n",
        "Compilado, vamos treinar o nosso modelo!"
      ],
      "metadata": {
        "id": "JNgEcPEz5CRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilando modelo e configurando o processo de treinamento\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=['accuracy'])\n",
        "# Treinando o modelo\n",
        "history = model.fit(x_train, y_train, epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78eGEUkH5H1m",
        "outputId": "cc49b4e7-bf05-4282-bd4b-106a01433e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 12s 936ms/step - loss: 1.2637 - accuracy: 0.4801 - val_loss: 1.2037 - val_accuracy: 0.5323\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 3s 546ms/step - loss: 1.1276 - accuracy: 0.5758 - val_loss: 1.2003 - val_accuracy: 0.5323\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 2s 435ms/step - loss: 1.1232 - accuracy: 0.5758 - val_loss: 1.1842 - val_accuracy: 0.5323\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 2s 414ms/step - loss: 1.1164 - accuracy: 0.5758 - val_loss: 1.2081 - val_accuracy: 0.5323\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 2s 435ms/step - loss: 1.1038 - accuracy: 0.5758 - val_loss: 1.1618 - val_accuracy: 0.5323\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 2s 429ms/step - loss: 1.0976 - accuracy: 0.5758 - val_loss: 1.1273 - val_accuracy: 0.5323\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 4s 717ms/step - loss: 1.0476 - accuracy: 0.5758 - val_loss: 1.0520 - val_accuracy: 0.6613\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 2s 438ms/step - loss: 0.9615 - accuracy: 0.6462 - val_loss: 0.9310 - val_accuracy: 0.6774\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 2s 421ms/step - loss: 0.7893 - accuracy: 0.7256 - val_loss: 0.8599 - val_accuracy: 0.6774\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 3s 666ms/step - loss: 0.7153 - accuracy: 0.7545 - val_loss: 0.8283 - val_accuracy: 0.6129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As variáveis epochs e batch_size que definimos no início do item 4., usamos no nosso modelo para treinamento. Abaixo o resultado do treinamento do modelo ao final das 10 épocas definidas."
      ],
      "metadata": {
        "id": "u9fqq29T5JMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avaliação do Modelo\n",
        "Para avaliar o resultado do nosso modelo utilizei a função evaluate, que retorna a função de perda e a acurácia do nosso modelo."
      ],
      "metadata": {
        "id": "2KOUJdUF5NoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testando a qualidade do modelo\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Joc8MME-5PLR",
        "outputId": "6196890d-49e1-4e24-f2ae-6579e8e4f9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 41ms/step - loss: 0.8283 - accuracy: 0.6129\n",
            "Test Loss: 0.8283111453056335\n",
            "Test Accuracy: 0.6129032373428345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predição com o Modelo\n",
        "Enfim o modelo está pronto, treinado e avaliado! Para testar novas entradas fiz a função sample_predict que retorna a probabilidade de cada uma das 4 classes."
      ],
      "metadata": {
        "id": "jCvDHNvv5SVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para predição\n",
        "def sample_predict(sample_pred_text):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "  return (predictions)"
      ],
      "metadata": {
        "id": "OkQmvqac5URF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com as probabilidades de cada classe, usamos a função argmax do numpy para retornar a posição com a probabilidade mais alta. Dessa forma podemos usar essa posição na função inverse_transform que retorna o label da nossa classe. Podemos ver isso no código abaixo."
      ],
      "metadata": {
        "id": "RB0ClmER5WJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição do exemplo\n",
        "example = 'feel better die happy'\n",
        "predictions = sample_predict(example)\n",
        "probabilities = [np.argmax(predictions[0])]\n",
        "# Retornando os labels\n",
        "new_label = label_encode.inverse_transform(probabilities)\n",
        "print('O exemplo \"{}\" foi classificado como \"{}\"'.format(example, new_label[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4-Kaw7N5Y2h",
        "outputId": "de590099-2b94-4833-ddaa-a5e1df07e51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "O exemplo \"feel better die happy\" foi classificado como \"Depression\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição do exemplo\n",
        "example = 'drugs beer drink'\n",
        "predictions = sample_predict(example)\n",
        "probabilities = [np.argmax(predictions[0])]\n",
        "# Retornando os labels\n",
        "new_label = label_encode.inverse_transform(probabilities)\n",
        "print('O exemplo \"{}\" foi classificado como \"{}\"'.format(example, new_label[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eUdOmXD-Iz5",
        "outputId": "a3cf46b0-b4ad-4c99-bcac-32d08bfb8e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 39ms/step\n",
            "O exemplo \"drugs beer drink\" foi classificado como \"Alcohol\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predição do exemplo\n",
        "example = 'no one will miss me'\n",
        "predictions = sample_predict(example)\n",
        "probabilities = [np.argmax(predictions[0])]\n",
        "# Retornando os labels\n",
        "new_label = label_encode.inverse_transform(probabilities)\n",
        "print('O exemplo \"{}\" foi classificado como \"{}\"'.format(example, new_label[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evbRN21m--nn",
        "outputId": "80d33dfa-761f-4521-f151-b8472516a4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "O exemplo \"no one will miss me\" foi classificado como \"Depression\"\n"
          ]
        }
      ]
    }
  ]
}